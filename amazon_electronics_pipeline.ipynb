{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6edc3f-184e-47c6-a4b8-5d61bd047c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0\nReady to go!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg, sum as spark_sum, desc, when\n",
    "\n",
    "# Verify Spark is ready (spark is pre-created in Databricks)\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbba4d0-67a8-43cc-975b-4a01c7910ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/Volumes/workspace/amazon_project/raw_data/Electronics.json.gz</td><td>Electronics.json.gz</td><td>3280683083</td><td>1762804017000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/Volumes/workspace/amazon_project/raw_data/Electronics.json.gz",
         "Electronics.json.gz",
         3280683083,
         1762804017000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify that file exists\n",
    "display(dbutils.fs.ls(\"/Volumes/workspace/amazon_project/raw_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498eccca-5a8a-43cf-8168-e6f2060d5b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.functions import col, avg, count, from_unixtime\n",
    "\n",
    "reviews_path = \"dbfs:/Volumes/workspace/amazon_project/raw_data/Electronics.json.gz\"\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"reviewerID\", T.StringType()),\n",
    "    T.StructField(\"asin\", T.StringType()),\n",
    "    T.StructField(\"overall\", T.DoubleType()),\n",
    "    T.StructField(\"reviewText\", T.StringType()),\n",
    "    T.StructField(\"summary\", T.StringType()),\n",
    "    T.StructField(\"unixReviewTime\", T.LongType()),\n",
    "    T.StructField(\"verified\", T.BooleanType()),\n",
    "    T.StructField(\"vote\", T.StringType())\n",
    "])\n",
    "\n",
    "reviews_raw = spark.read.schema(schema).json(reviews_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f790a9d4-e9b4-465b-8d5f-392cdf3e65ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step: Demonstrating Lazy Evaluation and Catalyst Optimization\n",
    "\n",
    "I will first apply transformations (filters, withColumn, and aggregation) in three separate cells to show that Spark builds a **logical plan** for each step but **does not execute anything** until an action (like `show()`, `count()`, or `write()`) is called.\n",
    "\n",
    "Later, I will chain them all together to show that Spark’s Catalyst optimizer combines these steps into a single optimized execution plan (predicate pushdown, column pruning, and stage fusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4278af-435b-4cbd-b659-2b6c3020ed00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(1) ColumnarToRow\n+- PhotonResultStage\n   +- PhotonFilter (((((verified#12658 AND isnotnull(verified#12658)) AND isnotnull(overall#12654)) AND (overall#12654 >= 1.0)) AND (overall#12654 <= 5.0)) AND (length(trim(reviewText#12655, None)) > 0))\n      +- PhotonJsonScan json [reviewerID#12652,asin#12653,overall#12654,reviewText#12655,summary#12656,unixReviewTime#12657L,verified#12658,vote#12659] Batched: true, DataFilters: [verified#12658, isnotnull(verified#12658), isnotnull(overall#12654), (overall#12654 >= 1.0), (ov..., Format: JSON, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/workspace/amazon_project/raw_data/Electronics.json.gz], PartitionFilters: [], PushedFilters: [EqualTo(verified,true), IsNotNull(verified), IsNotNull(overall), GreaterThanOrEqual(overall,1.0)..., ReadSchema: struct<reviewerID:string,asin:string,overall:double,reviewText:string,summary:string,unixReviewTi...\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "reviews_filtered = (\n",
    "    reviews_raw\n",
    "      .filter((col(\"verified\") == True))\n",
    "      .filter((col(\"overall\") >= 1) & (col(\"overall\") <= 5))\n",
    "      .filter(F.length(F.trim(F.col(\"reviewText\"))) > 0)\n",
    ")\n",
    "\n",
    "# No action yet — Spark hasn't executed\n",
    "reviews_filtered.explain()  # Logical plan only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4631bbfe-141a-4284-b54b-6dd4d0dfe798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(1) ColumnarToRow\n+- PhotonResultStage\n   +- PhotonProject [reviewerID#12652, asin#12653, overall#12654, reviewText#12655, summary#12656, unixReviewTime#12657L, verified#12658, vote#12659, vote_clean#12664, vote_int#12666, review_len#12668, review_ts#12670, year(cast(review_ts#12670 as date)) AS review_year#12672, date_format(review_ts#12670, yyyy-MM, Some(Etc/UTC)) AS review_month#12674]\n      +- PhotonProject [reviewerID#12652, asin#12653, overall#12654, reviewText#12655, summary#12656, unixReviewTime#12657L, verified#12658, vote#12659, vote_clean#12664, CASE WHEN RLIKE(vote_clean#12664, ^[0-9]+$) THEN cast(vote_clean#12664 as int) ELSE 0 END AS vote_int#12666, length(reviewText#12655) AS review_len#12668, cast(from_unixtime(unixReviewTime#12657L, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) as timestamp) AS review_ts#12670]\n         +- PhotonProject [reviewerID#12652, asin#12653, overall#12654, reviewText#12655, summary#12656, unixReviewTime#12657L, verified#12658, vote#12659, replace(vote#12659, ,, ) AS vote_clean#12664]\n            +- PhotonFilter (((((verified#12658 AND isnotnull(verified#12658)) AND isnotnull(overall#12654)) AND (overall#12654 >= 1.0)) AND (overall#12654 <= 5.0)) AND (length(trim(reviewText#12655, None)) > 0))\n               +- PhotonJsonScan json [reviewerID#12652,asin#12653,overall#12654,reviewText#12655,summary#12656,unixReviewTime#12657L,verified#12658,vote#12659] Batched: true, DataFilters: [verified#12658, isnotnull(verified#12658), isnotnull(overall#12654), (overall#12654 >= 1.0), (ov..., Format: JSON, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/workspace/amazon_project/raw_data/Electronics.json.gz], PartitionFilters: [], PushedFilters: [EqualTo(verified,true), IsNotNull(verified), IsNotNull(overall), GreaterThanOrEqual(overall,1.0)..., ReadSchema: struct<reviewerID:string,asin:string,overall:double,reviewText:string,summary:string,unixReviewTi...\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "reviews_transformed = (\n",
    "    reviews_filtered\n",
    "      .withColumn(\"vote_clean\", F.regexp_replace(\"vote\", \",\", \"\"))\n",
    "      .withColumn(\"vote_int\", F.when(F.col(\"vote_clean\").rlike(\"^[0-9]+$\"), F.col(\"vote_clean\").cast(\"int\")).otherwise(0))\n",
    "      .withColumn(\"review_len\", F.length(\"reviewText\"))\n",
    "      .withColumn(\"review_ts\", F.from_unixtime(\"unixReviewTime\").cast(\"timestamp\"))\n",
    "      .withColumn(\"review_year\", F.year(\"review_ts\"))\n",
    "      .withColumn(\"review_month\", F.date_format(\"review_ts\", \"yyyy-MM\"))\n",
    ")\n",
    "reviews_transformed.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a130c133-5ddb-41f9-9546-379822f4c23e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(1) ColumnarToRow\n+- PhotonResultStage\n   +- PhotonProject [reviewerID#12652, asin#12653, overall#12654, reviewText#12655, summary#12656, unixReviewTime#12657L, verified#12658, vote#12659, vote_clean#12678, vote_int#12680, review_len#12682, review_ts#12684, year(cast(review_ts#12684 as date)) AS review_year#12686, date_format(review_ts#12684, yyyy-MM, Some(Etc/UTC)) AS review_month#12688]\n      +- PhotonProject [reviewerID#12652, asin#12653, overall#12654, reviewText#12655, summary#12656, unixReviewTime#12657L, verified#12658, vote#12659, vote_clean#12678, CASE WHEN RLIKE(vote_clean#12678, ^[0-9]+$) THEN cast(vote_clean#12678 as int) ELSE 0 END AS vote_int#12680, length(reviewText#12655) AS review_len#12682, cast(from_unixtime(unixReviewTime#12657L, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) as timestamp) AS review_ts#12684]\n         +- PhotonProject [reviewerID#12652, asin#12653, overall#12654, reviewText#12655, summary#12656, unixReviewTime#12657L, verified#12658, vote#12659, replace(vote#12659, ,, ) AS vote_clean#12678]\n            +- PhotonFilter (((((verified#12658 AND isnotnull(verified#12658)) AND isnotnull(overall#12654)) AND (overall#12654 >= 1.0)) AND (overall#12654 <= 5.0)) AND (length(trim(reviewText#12655, None)) > 0))\n               +- PhotonJsonScan json [reviewerID#12652,asin#12653,overall#12654,reviewText#12655,summary#12656,unixReviewTime#12657L,verified#12658,vote#12659] Batched: true, DataFilters: [verified#12658, isnotnull(verified#12658), isnotnull(overall#12654), (overall#12654 >= 1.0), (ov..., Format: JSON, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/workspace/amazon_project/raw_data/Electronics.json.gz], PartitionFilters: [], PushedFilters: [EqualTo(verified,true), IsNotNull(verified), IsNotNull(overall), GreaterThanOrEqual(overall,1.0)..., ReadSchema: struct<reviewerID:string,asin:string,overall:double,reviewText:string,summary:string,unixReviewTi...\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "reviews_transformed = (\n",
    "    reviews_filtered\n",
    "      .withColumn(\"vote_clean\", F.regexp_replace(\"vote\", \",\", \"\"))\n",
    "      .withColumn(\"vote_int\", F.when(F.col(\"vote_clean\").rlike(\"^[0-9]+$\"), F.col(\"vote_clean\").cast(\"int\")).otherwise(0))\n",
    "      .withColumn(\"review_len\", F.length(\"reviewText\"))\n",
    "      .withColumn(\"review_ts\", F.from_unixtime(\"unixReviewTime\").cast(\"timestamp\"))\n",
    "      .withColumn(\"review_year\", F.year(\"review_ts\"))\n",
    "      .withColumn(\"review_month\", F.date_format(\"review_ts\", \"yyyy-MM\"))\n",
    ")\n",
    "reviews_transformed.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4c8e63-581c-4c45-a86e-76718bd99382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonGroupingAgg(keys=[review_year#12686], functions=[finalmerge_count(merge count#12700L) AS count(1)#12695L, finalmerge_avg(merge sum#12703, count#12704L) AS avg(overall)#12696, finalmerge_avg(merge sum#12707, count#12708L) AS avg(review_len)#12697, finalmerge_sum(merge sum#12710L) AS sum(vote_int)#12698L])\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#11799]\n               +- PhotonShuffleExchangeSink hashpartitioning(review_year#12686, 1024)\n                  +- PhotonGroupingAgg(keys=[review_year#12686], functions=[partial_count(1) AS count#12700L, partial_avg(overall#12654) AS (sum#12703, count#12704L), partial_avg(review_len#12682) AS (sum#12707, count#12708L), partial_sum(vote_int#12680) AS sum#12710L])\n                     +- PhotonProject [overall#12654, CASE WHEN RLIKE(vote_clean#12678, ^[0-9]+$) THEN cast(vote_clean#12678 as int) ELSE 0 END AS vote_int#12680, length(reviewText#12655) AS review_len#12682, year(cast(cast(from_unixtime(unixReviewTime#12657L, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) as timestamp) as date)) AS review_year#12686]\n                        +- PhotonProject [overall#12654, reviewText#12655, unixReviewTime#12657L, replace(vote#12659, ,, ) AS vote_clean#12678]\n                           +- PhotonFilter (((((verified#12658 AND isnotnull(verified#12658)) AND isnotnull(overall#12654)) AND (overall#12654 >= 1.0)) AND (overall#12654 <= 5.0)) AND (length(trim(reviewText#12655, None)) > 0))\n                              +- PhotonJsonScan json [overall#12654,reviewText#12655,unixReviewTime#12657L,verified#12658,vote#12659] Batched: true, DataFilters: [verified#12658, isnotnull(verified#12658), isnotnull(overall#12654), (overall#12654 >= 1.0), (ov..., Format: JSON, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/workspace/amazon_project/raw_data/Electronics.json.gz], PartitionFilters: [], PushedFilters: [EqualTo(verified,true), IsNotNull(verified), IsNotNull(overall), GreaterThanOrEqual(overall,1.0)..., ReadSchema: struct<overall:double,reviewText:string,unixReviewTime:bigint,verified:boolean,vote:string>\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "reviews_agg = (\n",
    "    reviews_transformed\n",
    "      .groupBy(\"review_year\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"n_reviews\"),\n",
    "          F.avg(\"overall\").alias(\"avg_rating\"),\n",
    "          F.avg(\"review_len\").alias(\"avg_length\"),\n",
    "          F.sum(\"vote_int\").alias(\"total_votes\")\n",
    "      )\n",
    ")\n",
    "reviews_agg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983d2ab7-5a80-473f-ae1d-6687e7f2e5e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_year</th><th>n_reviews</th><th>avg_rating</th><th>avg_length</th><th>total_votes</th></tr></thead><tbody><tr><td>2018</td><td>1414803</td><td>4.065596411655898</td><td>167.2316944479196</td><td>159073</td></tr><tr><td>2006</td><td>22949</td><td>4.055775850799599</td><td>687.4453353087281</td><td>202200</td></tr><tr><td>2000</td><td>666</td><td>4.105105105105105</td><td>771.0360360360361</td><td>11596</td></tr><tr><td>2007</td><td>73618</td><td>4.191556412833818</td><td>564.9701024206037</td><td>391605</td></tr><tr><td>2013</td><td>1738860</td><td>4.121423231312469</td><td>333.85820020013114</td><td>1767327</td></tr><tr><td>2005</td><td>11396</td><td>3.922867672867673</td><td>779.845121095121</td><td>138701</td></tr><tr><td>2009</td><td>194218</td><td>4.125446663028144</td><td>587.9962155927875</td><td>736769</td></tr><tr><td>2008</td><td>117056</td><td>4.191694573537452</td><td>611.1936423592127</td><td>569917</td></tr><tr><td>2015</td><td>3922624</td><td>4.1340120796691195</td><td>185.7321027964954</td><td>2276337</td></tr><tr><td>2003</td><td>3286</td><td>3.9269628727936703</td><td>774.6877662811929</td><td>33579</td></tr><tr><td>2010</td><td>302077</td><td>4.0282014188435395</td><td>542.882860992396</td><td>890085</td></tr><tr><td>2017</td><td>2925937</td><td>4.091403881901764</td><td>176.9171031365337</td><td>1090492</td></tr><tr><td>1999</td><td>58</td><td>4.120689655172414</td><td>687.8620689655172</td><td>1869</td></tr><tr><td>2001</td><td>1664</td><td>3.9969951923076925</td><td>715.9567307692307</td><td>16332</td></tr><tr><td>2004</td><td>4403</td><td>3.7996820349761524</td><td>917.4029071087895</td><td>67422</td></tr><tr><td>2012</td><td>756576</td><td>4.069792063189951</td><td>456.1972293596413</td><td>1464578</td></tr><tr><td>2014</td><td>2567839</td><td>4.1364957070906705</td><td>247.43348551057912</td><td>1863361</td></tr><tr><td>2011</td><td>455268</td><td>4.016840630134339</td><td>527.9247498176898</td><td>1162197</td></tr><tr><td>2016</td><td>4072456</td><td>4.114984422176692</td><td>192.36104601252904</td><td>2471430</td></tr><tr><td>2002</td><td>2231</td><td>3.9843119677274763</td><td>758.1779471089197</td><td>20715</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2018,
         1414803,
         4.065596411655898,
         167.2316944479196,
         159073
        ],
        [
         2006,
         22949,
         4.055775850799599,
         687.4453353087281,
         202200
        ],
        [
         2000,
         666,
         4.105105105105105,
         771.0360360360361,
         11596
        ],
        [
         2007,
         73618,
         4.191556412833818,
         564.9701024206037,
         391605
        ],
        [
         2013,
         1738860,
         4.121423231312469,
         333.85820020013114,
         1767327
        ],
        [
         2005,
         11396,
         3.922867672867673,
         779.845121095121,
         138701
        ],
        [
         2009,
         194218,
         4.125446663028144,
         587.9962155927875,
         736769
        ],
        [
         2008,
         117056,
         4.191694573537452,
         611.1936423592127,
         569917
        ],
        [
         2015,
         3922624,
         4.1340120796691195,
         185.7321027964954,
         2276337
        ],
        [
         2003,
         3286,
         3.9269628727936703,
         774.6877662811929,
         33579
        ],
        [
         2010,
         302077,
         4.0282014188435395,
         542.882860992396,
         890085
        ],
        [
         2017,
         2925937,
         4.091403881901764,
         176.9171031365337,
         1090492
        ],
        [
         1999,
         58,
         4.120689655172414,
         687.8620689655172,
         1869
        ],
        [
         2001,
         1664,
         3.9969951923076925,
         715.9567307692307,
         16332
        ],
        [
         2004,
         4403,
         3.7996820349761524,
         917.4029071087895,
         67422
        ],
        [
         2012,
         756576,
         4.069792063189951,
         456.1972293596413,
         1464578
        ],
        [
         2014,
         2567839,
         4.1364957070906705,
         247.43348551057912,
         1863361
        ],
        [
         2011,
         455268,
         4.016840630134339,
         527.9247498176898,
         1162197
        ],
        [
         2016,
         4072456,
         4.114984422176692,
         192.36104601252904,
         2471430
        ],
        [
         2002,
         2231,
         3.9843119677274763,
         758.1779471089197,
         20715
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "n_reviews",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_rating",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_votes",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This triggers the actual Spark job\n",
    "display(reviews_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa48f43-9046-4b55-a19f-e660fc9a6f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan (11)\n+- == Initial Plan ==\n   ColumnarToRow (10)\n   +- PhotonResultStage (9)\n      +- PhotonGroupingAgg (8)\n         +- PhotonShuffleExchangeSource (7)\n            +- PhotonShuffleMapStage (6)\n               +- PhotonShuffleExchangeSink (5)\n                  +- PhotonGroupingAgg (4)\n                     +- PhotonProject (3)\n                        +- PhotonFilter (2)\n                           +- PhotonJsonScan json  (1)\n\n\n(1) PhotonJsonScan json \nOutput [3]: [overall#12806, unixReviewTime#12809L, verified#12810]\nBatched: true\nLocation: InMemoryFileIndex [dbfs:/Volumes/workspace/amazon_project/raw_data/Electronics.json.gz]\nPushedFilters: [EqualTo(verified,true), IsNotNull(verified), IsNotNull(overall), GreaterThanOrEqual(overall,1.0), LessThanOrEqual(overall,5.0)]\nReadSchema: struct<overall:double,unixReviewTime:bigint,verified:boolean>\n\n(2) PhotonFilter\nInput [3]: [overall#12806, unixReviewTime#12809L, verified#12810]\nArguments: ((((verified#12810 AND isnotnull(verified#12810)) AND isnotnull(overall#12806)) AND (overall#12806 >= 1.0)) AND (overall#12806 <= 5.0))\n\n(3) PhotonProject\nInput [3]: [overall#12806, unixReviewTime#12809L, verified#12810]\nArguments: [overall#12806, year(cast(from_unixtime(unixReviewTime#12809L, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) as date)) AS review_year#12820]\n\n(4) PhotonGroupingAgg\nInput [2]: [overall#12806, review_year#12820]\nArguments: [review_year#12820], [partial_count(1) AS count#12826L, partial_avg(overall#12806) AS (sum#12829, count#12830L)], [count#12825L, sum#12827, count#12828L], [review_year#12820, count#12826L, sum#12829, count#12830L], false\n\n(5) PhotonShuffleExchangeSink\nInput [4]: [review_year#12820, count#12826L, sum#12829, count#12830L]\nArguments: hashpartitioning(review_year#12820, 1024)\n\n(6) PhotonShuffleMapStage\nInput [4]: [review_year#12820, count#12826L, sum#12829, count#12830L]\nArguments: ENSURE_REQUIREMENTS, [id=#12065]\n\n(7) PhotonShuffleExchangeSource\nInput [4]: [review_year#12820, count#12826L, sum#12829, count#12830L]\n\n(8) PhotonGroupingAgg\nInput [4]: [review_year#12820, count#12826L, sum#12829, count#12830L]\nArguments: [review_year#12820], [finalmerge_count(merge count#12826L) AS count(1)#12823L, finalmerge_avg(merge sum#12829, count#12830L) AS avg(overall)#12824], [count(1)#12823L, avg(overall)#12824], [review_year#12820, count(1)#12823L AS n_reviews#12821L, avg(overall)#12824 AS avg_rating#12822], true\n\n(9) PhotonResultStage\nInput [3]: [review_year#12820, n_reviews#12821L, avg_rating#12822]\n\n(10) ColumnarToRow\nInput [3]: [review_year#12820, n_reviews#12821L, avg_rating#12822]\n\n(11) AdaptiveSparkPlan\nOutput [3]: [review_year#12820, n_reviews#12821L, avg_rating#12822]\nArguments: isFinalPlan=false\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "reviews_pipeline = (\n",
    "    spark.read.schema(schema).json(reviews_path)\n",
    "      .filter(F.col(\"verified\") == True)\n",
    "      .filter((F.col(\"overall\") >= 1) & (F.col(\"overall\") <= 5))\n",
    "      .withColumn(\"vote_clean\", F.regexp_replace(\"vote\", \",\", \"\"))\n",
    "      .withColumn(\"vote_int\", F.when(F.col(\"vote_clean\").rlike(\"^[0-9]+$\"), F.col(\"vote_clean\").cast(\"int\")).otherwise(0))\n",
    "      .withColumn(\"review_len\", F.length(\"reviewText\"))\n",
    "      .withColumn(\"review_year\", F.year(F.from_unixtime(\"unixReviewTime\")))\n",
    "      .groupBy(\"review_year\")\n",
    "      .agg(F.count(\"*\").alias(\"n_reviews\"), F.avg(\"overall\").alias(\"avg_rating\"))\n",
    ")\n",
    "reviews_pipeline.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26b97b6-ea5f-4861-a647-171aba4b9120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0b85d7-f69c-43f7-888e-e16b6fef2dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL view for Spark SQL\n",
    "reviews_raw.createOrReplaceTempView(\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ca0342-302d-407d-9883-e12229eeb6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### SQL Query 1 – Average rating by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c735e0c6-d5ca-4f56-bd23-bc67e119d6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_year</th><th>n_reviews</th><th>avg_rating</th></tr></thead><tbody><tr><td>1999</td><td>58</td><td>4.12</td></tr><tr><td>2000</td><td>666</td><td>4.11</td></tr><tr><td>2001</td><td>1664</td><td>4.0</td></tr><tr><td>2002</td><td>2231</td><td>3.98</td></tr><tr><td>2003</td><td>3286</td><td>3.93</td></tr><tr><td>2004</td><td>4403</td><td>3.8</td></tr><tr><td>2005</td><td>11396</td><td>3.92</td></tr><tr><td>2006</td><td>22949</td><td>4.06</td></tr><tr><td>2007</td><td>73618</td><td>4.19</td></tr><tr><td>2008</td><td>117056</td><td>4.19</td></tr><tr><td>2009</td><td>194218</td><td>4.13</td></tr><tr><td>2010</td><td>302077</td><td>4.03</td></tr><tr><td>2011</td><td>455268</td><td>4.02</td></tr><tr><td>2012</td><td>756578</td><td>4.07</td></tr><tr><td>2013</td><td>1738874</td><td>4.12</td></tr><tr><td>2014</td><td>2568269</td><td>4.14</td></tr><tr><td>2015</td><td>3924783</td><td>4.13</td></tr><tr><td>2016</td><td>4075225</td><td>4.12</td></tr><tr><td>2017</td><td>2928306</td><td>4.09</td></tr><tr><td>2018</td><td>1416167</td><td>4.07</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1999,
         58,
         4.12
        ],
        [
         2000,
         666,
         4.11
        ],
        [
         2001,
         1664,
         4.0
        ],
        [
         2002,
         2231,
         3.98
        ],
        [
         2003,
         3286,
         3.93
        ],
        [
         2004,
         4403,
         3.8
        ],
        [
         2005,
         11396,
         3.92
        ],
        [
         2006,
         22949,
         4.06
        ],
        [
         2007,
         73618,
         4.19
        ],
        [
         2008,
         117056,
         4.19
        ],
        [
         2009,
         194218,
         4.13
        ],
        [
         2010,
         302077,
         4.03
        ],
        [
         2011,
         455268,
         4.02
        ],
        [
         2012,
         756578,
         4.07
        ],
        [
         2013,
         1738874,
         4.12
        ],
        [
         2014,
         2568269,
         4.14
        ],
        [
         2015,
         3924783,
         4.13
        ],
        [
         2016,
         4075225,
         4.12
        ],
        [
         2017,
         2928306,
         4.09
        ],
        [
         2018,
         1416167,
         4.07
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "review_year",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "n_reviews",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "avg_rating",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 180
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "n_reviews",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_rating",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- SQL Query 1: Yearly average rating\n",
    "SELECT \n",
    "  YEAR(FROM_UNIXTIME(unixReviewTime)) AS review_year,\n",
    "  COUNT(*) AS n_reviews,\n",
    "  ROUND(AVG(overall), 2) AS avg_rating\n",
    "FROM reviews\n",
    "WHERE verified = true\n",
    "GROUP BY review_year\n",
    "ORDER BY review_year;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66968828-faab-4f72-9c33-049964b6ae8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### SQL Query 2 – Top products (ASINs) with ≥1000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae79f1e-c043-429d-9908-59c4ec650e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>asin</th><th>n_reviews</th><th>avg_rating</th></tr></thead><tbody><tr><td>B01DBV1OKY</td><td>2651</td><td>4.87</td></tr><tr><td>B00RJBWA9C</td><td>2255</td><td>4.87</td></tr><tr><td>B01BV2KXYI</td><td>1385</td><td>4.85</td></tr><tr><td>B001AQYJI2</td><td>1175</td><td>4.84</td></tr><tr><td>B004Y1AYAC</td><td>1626</td><td>4.84</td></tr><tr><td>B00KPRWAX8</td><td>1260</td><td>4.82</td></tr><tr><td>B005LS2HM0</td><td>1325</td><td>4.82</td></tr><tr><td>B0029N3U8K</td><td>1162</td><td>4.82</td></tr><tr><td>B00DI89YAI</td><td>2376</td><td>4.82</td></tr><tr><td>B0043WJRRS</td><td>10345</td><td>4.81</td></tr><tr><td>B001MSU1HG</td><td>3163</td><td>4.81</td></tr><tr><td>B01054S5FM</td><td>2562</td><td>4.81</td></tr><tr><td>B018TQRUWW</td><td>1266</td><td>4.81</td></tr><tr><td>B00125Y0NU</td><td>1160</td><td>4.81</td></tr><tr><td>B000WU2LXC</td><td>3396</td><td>4.81</td></tr><tr><td>B00YG14W4O</td><td>2857</td><td>4.8</td></tr><tr><td>B005LJQO9G</td><td>5378</td><td>4.8</td></tr><tr><td>B006U3O566</td><td>3161</td><td>4.8</td></tr><tr><td>B00YG1FNVA</td><td>3451</td><td>4.8</td></tr><tr><td>B00WF78GS4</td><td>1189</td><td>4.8</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "B01DBV1OKY",
         2651,
         4.87
        ],
        [
         "B00RJBWA9C",
         2255,
         4.87
        ],
        [
         "B01BV2KXYI",
         1385,
         4.85
        ],
        [
         "B001AQYJI2",
         1175,
         4.84
        ],
        [
         "B004Y1AYAC",
         1626,
         4.84
        ],
        [
         "B00KPRWAX8",
         1260,
         4.82
        ],
        [
         "B005LS2HM0",
         1325,
         4.82
        ],
        [
         "B0029N3U8K",
         1162,
         4.82
        ],
        [
         "B00DI89YAI",
         2376,
         4.82
        ],
        [
         "B0043WJRRS",
         10345,
         4.81
        ],
        [
         "B001MSU1HG",
         3163,
         4.81
        ],
        [
         "B01054S5FM",
         2562,
         4.81
        ],
        [
         "B018TQRUWW",
         1266,
         4.81
        ],
        [
         "B00125Y0NU",
         1160,
         4.81
        ],
        [
         "B000WU2LXC",
         3396,
         4.81
        ],
        [
         "B00YG14W4O",
         2857,
         4.8
        ],
        [
         "B005LJQO9G",
         5378,
         4.8
        ],
        [
         "B006U3O566",
         3161,
         4.8
        ],
        [
         "B00YG1FNVA",
         3451,
         4.8
        ],
        [
         "B00WF78GS4",
         1189,
         4.8
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "asin",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "n_reviews",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "avg_rating",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 181
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "asin",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n_reviews",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_rating",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- SQL Query 2: Top ASINs by number of reviews and average rating\n",
    "SELECT \n",
    "  asin,\n",
    "  COUNT(*) AS n_reviews,\n",
    "  ROUND(AVG(overall), 2) AS avg_rating\n",
    "FROM reviews\n",
    "WHERE verified = true\n",
    "GROUP BY asin\n",
    "HAVING COUNT(*) >= 1000\n",
    "ORDER BY avg_rating DESC\n",
    "LIMIT 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6fa7eda-03d9-4297-ac33-668144b2cb57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4551495771618192>, line 8\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m external_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m (\n",
       "\u001B[1;32m      4\u001B[0m     spark_agg\n",
       "\u001B[1;32m      5\u001B[0m       \u001B[38;5;241m.\u001B[39mrepartition(\u001B[38;5;241m32\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m       \u001B[38;5;241m.\u001B[39mwrite\n",
       "\u001B[1;32m      7\u001B[0m       \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 8\u001B[0m       \u001B[38;5;241m.\u001B[39mparquet(external_path)\n",
       "\u001B[1;32m      9\u001B[0m )\n",
       "\u001B[1;32m     11\u001B[0m display(dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mls(external_path))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:779\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n",
       "\u001B[1;32m    777\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n",
       "\u001B[1;32m    778\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n",
       "\u001B[0;32m--> 779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:703\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    701\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n",
       "\u001B[0;32m--> 703\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    705\u001B[0m )\n",
       "\u001B[1;32m    706\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mUnknownException\u001B[0m: (java.nio.file.AccessDeniedException) s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/_delta_log: getFileStatus on s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null), S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=:403 Forbidden\n",
       "\n",
       "JVM stacktrace:\n",
       "java.nio.file.AccessDeniedException\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:292)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4483)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n",
       "\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n",
       "\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n",
       "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\n",
       "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksS3Client.getObjectMetadata(DatabricksS3Client.scala:121)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2678)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:435)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:394)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2668)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2636)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4467)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n",
       "\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n",
       "\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n",
       "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "UnknownException",
        "evalue": "(java.nio.file.AccessDeniedException) s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/_delta_log: getFileStatus on s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null), S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=:403 Forbidden\n\nJVM stacktrace:\njava.nio.file.AccessDeniedException\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:292)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4483)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksS3Client.getObjectMetadata(DatabricksS3Client.scala:121)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2678)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:435)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:394)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2668)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2636)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4467)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>UnknownException</span>: (java.nio.file.AccessDeniedException) s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/_delta_log: getFileStatus on s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null), S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=:403 Forbidden\n\nJVM stacktrace:\njava.nio.file.AccessDeniedException\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:292)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4483)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksS3Client.getObjectMetadata(DatabricksS3Client.scala:121)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2678)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:435)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:394)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2668)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2636)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4467)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XXKCM",
        "stackTrace": "java.nio.file.AccessDeniedException\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:292)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4483)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksS3Client.getObjectMetadata(DatabricksS3Client.scala:121)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2678)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:435)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:394)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2668)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2636)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4467)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-4551495771618192>, line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m external_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      3\u001B[0m (\n\u001B[1;32m      4\u001B[0m     spark_agg\n\u001B[1;32m      5\u001B[0m       \u001B[38;5;241m.\u001B[39mrepartition(\u001B[38;5;241m32\u001B[39m)\n\u001B[1;32m      6\u001B[0m       \u001B[38;5;241m.\u001B[39mwrite\n\u001B[1;32m      7\u001B[0m       \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m       \u001B[38;5;241m.\u001B[39mparquet(external_path)\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     11\u001B[0m display(dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mls(external_path))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:779\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n\u001B[0;32m--> 779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:703\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    701\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m--> 703\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    705\u001B[0m )\n\u001B[1;32m    706\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mUnknownException\u001B[0m: (java.nio.file.AccessDeniedException) s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/_delta_log: getFileStatus on s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null), S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=:403 Forbidden\n\nJVM stacktrace:\njava.nio.file.AccessDeniedException\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:292)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4483)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0.s3.us-east-1.amazonaws.com _delta_log {} Hadoop 3.4.1, aws-sdk-java/1.12.638 Linux/5.15.0-1091-aws OpenJDK_64-Bit_Server_VM/17.0.16+8-LTS java/17.0.16 scala/2.13.16 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: FRWYXSAMPZXMW3GF, Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=, Cloud Provider: AWS, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: FRWYXSAMPZXMW3GF; S3 Extended Request ID: ymVxNeM7hmTPID+drgakoWdRShbye6Qx5K5vx+4Dt5Q20FTU7UElWhJfx6uvkuLsBr3OfNqGU9IrQV4/5kI8wbue9rbtPgjPR6KnBKYRXe8=; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksS3Client.getObjectMetadata(DatabricksS3Client.scala:121)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2678)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:435)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:394)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2668)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2636)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4467)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4412)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4292)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:82)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:72)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:251)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:332)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3279)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "external_path = \"s3://databricks-s3-ingest-95ac0-lambdazipsbucket-jniyxzmi4zy0/amazon_project/output\"\n",
    "\n",
    "(\n",
    "    spark_agg\n",
    "      .repartition(32)\n",
    "      .write\n",
    "      .mode(\"overwrite\")\n",
    "      .parquet(external_path)\n",
    ")\n",
    "\n",
    "display(dbutils.fs.ls(external_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397ba131-383a-4b15-8870-b6d7a5423521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Caching optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa3dc08-a863-4d7d-a1f7-ea24c8e1b673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First run: 0.06s\nSecond run: 0.06s (should be faster)\n"
     ]
    }
   ],
   "source": [
    "# Materialize the DataFrame as a temporary view\n",
    "reviews_spark_filter.createOrReplaceTempView(\"reviews_spark_filter_temp\")\n",
    "\n",
    "# Use SQL to count rows (will be faster after materialization)\n",
    "t0 = time.time()\n",
    "spark.sql(\"SELECT COUNT(*) FROM reviews_spark_filter_temp\")\n",
    "t1 = time.time()\n",
    "\n",
    "# Repeat to demonstrate improved performance\n",
    "t2 = time.time()\n",
    "spark.sql(\"SELECT COUNT(*) FROM reviews_spark_filter_temp\")\n",
    "t3 = time.time()\n",
    "\n",
    "print(f\"First run: {t1 - t0:.2f}s\")\n",
    "print(f\"Second run: {t3 - t2:.2f}s (should be faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a47da64-3e4e-4906-b7fa-ffd115808d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Actions vs Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b800fa-c867-41e9-91f0-3ff40b9e1b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Lazy Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc298f38-1833-41f9-8694-078071be411c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(1) ColumnarToRow\n+- PhotonResultStage\n   +- PhotonProject [id#13296L, squared#13298, CASE WHEN (squared#13298 > 20.0) THEN high ELSE low END AS category#13300]\n      +- PhotonProject [id#13296L, POWER(cast(id#13296L as double), 2.0) AS squared#13298]\n         +- PhotonFilter ((id#13296L % 2) = 0)\n            +- PhotonRange Range (0, 10, step=1, splits=8)\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start with a small example DataFrame\n",
    "demo_df = spark.range(0, 10)  # creates a DataFrame with a single column 'id' = [0,1,2,...,9]\n",
    "\n",
    "# --- Transformations (Lazy) ---\n",
    "transformed_df = (\n",
    "    demo_df\n",
    "      .filter(F.col(\"id\") % 2 == 0)       # Keep even numbers\n",
    "      .withColumn(\"squared\", F.col(\"id\") ** 2)  # Compute a new column\n",
    "      .withColumn(\"category\", F.when(F.col(\"squared\") > 20, \"high\").otherwise(\"low\"))\n",
    ")\n",
    "\n",
    "# Print the logical/physical plan, but note: no job has run yet!\n",
    "transformed_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3cd6d4b-1150-4943-911a-8b08185501d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8519e18-204d-440b-9dd2-58a62854c5bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing transformed results:\n+---+-------+--------+\n| id|squared|category|\n+---+-------+--------+\n|  0|    0.0|     low|\n|  2|    4.0|     low|\n|  4|   16.0|     low|\n|  6|   36.0|    high|\n|  8|   64.0|    high|\n+---+-------+--------+\n\nNumber of rows in transformed_df: 5\n"
     ]
    }
   ],
   "source": [
    "# --- Actions (Eager) ---\n",
    "# These trigger Spark to actually compute results.\n",
    "print(\"Showing transformed results:\")\n",
    "transformed_df.show()\n",
    "\n",
    "# Another action example: count\n",
    "n_rows = transformed_df.count()\n",
    "print(f\"Number of rows in transformed_df: {n_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94af2cac-8884-4ed6-916b-ebbd6da3a848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2477a9-3232-4b86-94d4-7506e41f6f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>label</th><th>vote_int</th><th>review_len</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>712</td></tr><tr><td>1</td><td>0</td><td>2657</td></tr><tr><td>1</td><td>2</td><td>522</td></tr><tr><td>0</td><td>4</td><td>239</td></tr><tr><td>1</td><td>0</td><td>495</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         0,
         712
        ],
        [
         1,
         0,
         2657
        ],
        [
         1,
         2,
         522
        ],
        [
         0,
         4,
         239
        ],
        [
         1,
         0,
         495
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "label",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "vote_int",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "review_len",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- label: integer (nullable = false)\n |-- vote_int: integer (nullable = true)\n |-- review_len: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "ml_df = (\n",
    "    reviews_raw\n",
    "      .filter(F.col(\"verified\") == True)\n",
    "      .withColumn(\"label\", F.when(F.col(\"overall\") >= 4, 1).otherwise(0))  # target: positive vs negative\n",
    "      # clean commas and safely cast vote to int\n",
    "      .withColumn(\"vote_clean\", F.regexp_replace(\"vote\", \",\", \"\"))          # remove commas from strings like \"1,226\"\n",
    "      .withColumn(\n",
    "          \"vote_int\",\n",
    "          F.when(F.col(\"vote_clean\").rlike(\"^[0-9]+$\"), F.col(\"vote_clean\").cast(\"int\"))  # only cast if numeric\n",
    "           .otherwise(F.lit(0))\n",
    "      )\n",
    "      .withColumn(\"review_len\", F.length(\"reviewText\"))\n",
    "      .select(\"label\", \"vote_int\", \"review_len\")\n",
    "      .na.drop()\n",
    ")\n",
    "\n",
    "display(ml_df.limit(5))\n",
    "ml_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4773c631-7657-4609-8340-e2387c9f0b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"vote_int\", \"review_len\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Split into train/test sets\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ca02e0-1478-4a76-982f-8ef8819658c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44350bc4-8a11-4d92-bd61-6eebf994530d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>features</th><th>label</th><th>prediction</th><th>probability</th></tr></thead><tbody><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}</td><td>0</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"1.0\"]}",
         0,
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.21452940102831605\",\"0.7854705989716839\"]}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 2, \"attrs\": {\"numeric\": [{\"name\": \"vote_int\", \"idx\": 0}, {\"name\": \"review_len\", \"idx\": 1}]}}}",
         "name": "features",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        },
        {
         "metadata": "{}",
         "name": "label",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"ml_attr\": {\"num_vals\": 2, \"type\": \"nominal\"}}",
         "name": "prediction",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 2}}",
         "name": "probability",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.606\nAccuracy: 0.766\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_df)\n",
    "display(predictions.select(\"features\", \"label\", \"prediction\", \"probability\").limit(10))\n",
    "\n",
    "# Evaluate accuracy (area under ROC)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "accuracy = predictions.filter(F.col(\"label\") == F.col(\"prediction\")).count() / predictions.count()\n",
    "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79456068-4e69-4fe9-ac08-3934ba5cb7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4551495771618191,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "amazon_electronics_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}